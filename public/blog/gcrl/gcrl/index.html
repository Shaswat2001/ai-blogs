<!DOCTYPE html>
<html lang="en">
    <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="In normal Reinforcement Learning (RL), the agent is thrown into the world with states and actions. The policy $π(a∣s)$ just tries to figure out which action works best in each state to maximize rewards over time.
But here’s the problem: if the only reward comes at the very end, the agent is basically guessing in the dark until it accidentally stumbles upon success. Learning in this way can feel a lot like climbing a mountain without knowing how tall it is.
Imagine starting your hike, staring up at an endless slope, and not knowing when (or if) you’ll ever reach the top.
Most people would feel lost and give up.
" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="http://localhost:1313/blog/gcrl/gcrl/" />


    <title>
        
            From Blind Trial to Guided Journey: Understanding GCRL :: Shaswat&#39;s AI Blog  — Hello Friend NG Theme
        
    </title>





  <link rel="stylesheet" href="http://localhost:1313/main.min.07ea7ac7da67e2e153a7dfa2457bc6a19cca824288d175e223fadc579041bc51.css" integrity="sha256-B&#43;p6x9pn4uFTp9&#43;iRXvGoZzKgkKI0XXiI/rcV5BBvFE=" crossorigin="anonymous">





    <link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
    <link rel="manifest" href="http://localhost:1313/site.webmanifest">
    <link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="http://localhost:1313/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="From Blind Trial to Guided Journey: Understanding GCRL">
  <meta itemprop="description" content="In normal Reinforcement Learning (RL), the agent is thrown into the world with states and actions. The policy $π(a∣s)$ just tries to figure out which action works best in each state to maximize rewards over time.
But here’s the problem: if the only reward comes at the very end, the agent is basically guessing in the dark until it accidentally stumbles upon success. Learning in this way can feel a lot like climbing a mountain without knowing how tall it is.
Imagine starting your hike, staring up at an endless slope, and not knowing when (or if) you’ll ever reach the top.
Most people would feel lost and give up.">
  <meta itemprop="datePublished" content="2025-09-07T13:30:23-04:00">
  <meta itemprop="dateModified" content="2025-09-07T13:30:23-04:00">
  <meta itemprop="wordCount" content="656">
  <meta itemprop="image" content="http://localhost:1313/">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="http://localhost:1313/">
  <meta name="twitter:title" content="From Blind Trial to Guided Journey: Understanding GCRL">
  <meta name="twitter:description" content="In normal Reinforcement Learning (RL), the agent is thrown into the world with states and actions. The policy $π(a∣s)$ just tries to figure out which action works best in each state to maximize rewards over time.
But here’s the problem: if the only reward comes at the very end, the agent is basically guessing in the dark until it accidentally stumbles upon success. Learning in this way can feel a lot like climbing a mountain without knowing how tall it is.
Imagine starting your hike, staring up at an endless slope, and not knowing when (or if) you’ll ever reach the top.
Most people would feel lost and give up.">



    <meta property="og:url" content="http://localhost:1313/blog/gcrl/gcrl/">
  <meta property="og:site_name" content="Shaswat&#39;s AI Blog">
  <meta property="og:title" content="From Blind Trial to Guided Journey: Understanding GCRL">
  <meta property="og:description" content="In normal Reinforcement Learning (RL), the agent is thrown into the world with states and actions. The policy $π(a∣s)$ just tries to figure out which action works best in each state to maximize rewards over time.
But here’s the problem: if the only reward comes at the very end, the agent is basically guessing in the dark until it accidentally stumbles upon success. Learning in this way can feel a lot like climbing a mountain without knowing how tall it is.
Imagine starting your hike, staring up at an endless slope, and not knowing when (or if) you’ll ever reach the top.
Most people would feel lost and give up.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-09-07T13:30:23-04:00">
    <meta property="article:modified_time" content="2025-09-07T13:30:23-04:00">
    <meta property="og:image" content="http://localhost:1313/">






    <meta property="article:published_time" content="2025-09-07 13:30:23 -0400 EDT" />












    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="http://localhost:1313/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">&gt;</span>
            <span class="logo__text ">
                From Exploration to Exploitation</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
                <nav class="menu">
    <ul class="menu__inner"><li><a href="http://localhost:1313/blog/">Blog</a></li><li><a href="http://localhost:1313/misc/">Misc</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
                <span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
   <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
   3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
   13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
 </svg></span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="http://localhost:1313/blog/gcrl/gcrl/">From Blind Trial to Guided Journey: Understanding GCRL</a></h2>

            
            
            

            <div class="post-content">
                <p>In normal Reinforcement Learning (RL), the agent is thrown into the world with <strong>states</strong> and <strong>actions</strong>. The policy $π(a∣s)$ just tries to figure out which action works best in each state to maximize rewards over time.</p>
<p>But here’s the problem: if the only reward comes at the <strong>very end</strong>, the agent is basically <strong>guessing in the dark</strong> until it accidentally stumbles upon success. Learning in this way can feel a lot like <strong>climbing a mountain without knowing how tall it is</strong>.<br>
Imagine starting your hike, staring up at an endless slope, and not knowing when (or if) you’ll ever reach the top.<br>
Most people would feel lost and give up.</p>
<p>Similarly, an AI agent faced only with the final destination might just wander aimlessly, never really learning how to get there.</p>
<p><img src="http://localhost:1313/gcrl.png" alt="Checkpoint Analogy"></p>
<hr>
<h2 id="checkpoints-make-it-easier">Checkpoints Make It Easier</h2>
<p>Now, what if we made the climb easier?</p>
<p>Instead of aiming straight for the peak, we break the journey into <strong>smaller checkpoints</strong>—intermediate goals that are much easier to reach.</p>
<p>The agent doesn’t need to worry about the summit; it just has to make it to the next checkpoint.<br>
Step by step, those smaller wins add up to reaching the top. Suddenly, the policy becomes:</p>
<p>$$π(a∣s,g)$$</p>
<p>This means the agent chooses actions not just by looking at the current state $s$, but also by looking at the <strong>goal</strong> $g$.</p>
<p>In human terms: you’re not wandering up the mountain randomly—you’re following directions toward the next checkpoint.</p>
<hr>
<h2 id="what-is-goal-conditioned-rl">What is Goal-Conditioned RL?</h2>
<p>That’s the intuition behind <strong>Goal-Conditioned Reinforcement Learning (GCRL)</strong>.  Instead of only reasoning over states and actions, we also introduce <strong>goals</strong> as part of the learning process.</p>
<p>By teaching the policy to reach <strong>achievable subgoals</strong>, we make the overall problem much more tractable.</p>
<hr>
<h2 id="rewards-become-more-helpful">Rewards Become More Helpful</h2>
<p>Instead of waiting for one giant reward at the end, the agent gets <strong>small, meaningful rewards</strong> whenever it reaches a goal.</p>
<p>The math looks like this:</p>
<p>$$r(s,a,g) = 1 {\lVert ϕ(s′) − g \rVert ≤ ε}$$</p>
<p>Don’t worry about the symbols too much—this just says:<br>
<strong>“reward = 1 if you’re close enough to the goal, otherwise reward = 0.”</strong></p>
<p>Think of it like a <strong>beeping treasure detector</strong>: it only goes off when you’re within a small bubble around the treasure.</p>
<p>This makes the game way easier, because the agent gets feedback more often instead of waiting until the very end.</p>
<hr>
<h2 id="training-on-many-goals">Training on Many Goals</h2>
<p>Since we don’t just care about one goal, the agent is trained on <strong>many possible goals</strong>.<br>
The objective is:</p>
<p>$$J(π) = E_{g∼p_g}[ E[\sum^t γ^t r(s_t,a_t,g)]]$$</p>
<p>In words: the agent practices reaching random goals, collects rewards along the way, and learns a <strong>general policy</strong> that works for all of them.</p>
<p>This is powerful because solving smaller goals builds the skills needed to solve the bigger task later.</p>
<hr>
<h2 id="universal-value-functions">Universal Value Functions</h2>
<p>To help with planning, the agent also learns <strong>special value functions</strong> that depend on the goal.<br>
For example:</p>
<p>$$V_π(s,g) = E[\sum^t γ^t r(s_t,a_t,g)]$$</p>
<p>This is called a <strong>Universal Value Function</strong>, because it works for <strong>any goal</strong>.</p>
<p>Think of it as a <strong>scorecard</strong> that tells the agent:</p>
<blockquote>
<p>“If I’m here, how good is this state when I’m aiming for that goal?”</p></blockquote>
<hr>
<h2 id="goal-relabeling-learning-from-mistakes">Goal Relabeling: Learning from Mistakes</h2>
<p>And here’s the coolest part: even when the agent misses the goal, the experience isn’t wasted.</p>
<p>With a trick called <strong>goal relabeling</strong>, we pretend that <strong>wherever the agent ended up was the goal all along</strong>.</p>
<p>It’s like hiking toward checkpoint $A$ but accidentally reaching checkpoint $B$:<br>
well, you still practiced getting to $B$, and that’s valuable too.</p>
<p>This makes the learning <strong>super data-efficient</strong>, because every episode teaches the agent something useful.</p>
<hr>
<h2 id="why-gcrl-is-powerful">Why GCRL is Powerful</h2>
<p>So overall, Goal-Conditioned RL helps in two big ways:</p>
<ol>
<li><strong>More frequent, meaningful rewards</strong> – no more waiting until the end.</li>
<li><strong>Failed attempts become useful data</strong> – nothing is wasted.</li>
</ol>
<p>That’s why, compared to plain RL, it feels less like <strong>climbing an endless mountain</strong><br>
and more like a <strong>step-by-step journey with checkpoints guiding the way</strong>.</p>
<hr>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2025</span>
            <span><a href="http://localhost:1313/">Shaswat Garg</a></span>
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            <span><a href="http://localhost:1313/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
            
        </div>
    </div>
    
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span><span>Made with &#10084; by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
        </div>
    </div>
    
</footer>

            
        </div>

        



<script type="text/javascript" src="http://localhost:1313/bundle.min.ad54ad97364f77ede35def9096b162bb1f0b3973aa50b080f5e82fa147f6882e2a7200d7535adbf9b51bebf939f1c1ca9bbe6be87530092aca720eac4a226fda.js" integrity="sha512-rVStlzZPd&#43;3jXe&#43;QlrFiux8LOXOqULCA9egvoUf2iC4qcgDXU1rb&#43;bUb6/k58cHKm75r6HUwCSrKcg6sSiJv2g=="></script>




    </body>
</html>
